{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e632e86f",
      "metadata": {
        "id": "e632e86f"
      },
      "source": [
        "\n",
        "# ITAI 2373 — NewsBot Intelligence System (Midterm) — **BBC Dataset**\n",
        "**Student:** `Tzorin, Bryan`  \n",
        "\n",
        "**Repo folder:** `ITAI2373-NewsBot-Midterm/`  \n",
        "\n",
        "**Notebook name :** `NewsBot_Midterm_Tzorin_Bryan.ipynb`\n",
        "\n",
        "This notebook implements **Modules 1–8** end-to-end using the **BBC News** dataset from Kaggle. It is designed for **Google Colab (free tier)** and mirrors the midterm rubric.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92637e3a",
      "metadata": {
        "id": "92637e3a"
      },
      "source": [
        "## 0) Setup — installs & imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ed382713",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ed382713",
        "outputId": "f3c4a801-ebdf-48a6-e343-a2d3292b5452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ pandas available\n",
            "✓ numpy available\n",
            "Installing scikit-learn…\n",
            "✓ spacy available\n",
            "✓ nltk available\n",
            "✓ matplotlib available\n",
            "✓ seaborn available\n",
            "✅ Setup complete\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import sys, subprocess, os\n",
        "\n",
        "def ensure(pkg):\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "        print(f\"✓ {pkg} available\")\n",
        "    except Exception:\n",
        "        print(f\"Installing {pkg}…\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "for p in [\"pandas\", \"numpy\", \"scikit-learn\", \"spacy\", \"nltk\", \"matplotlib\", \"seaborn\"]:\n",
        "    ensure(p)\n",
        "\n",
        "import pandas as pd, numpy as np, re, json, math, os, random, collections\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report, confusion_matrix\n",
        "import nltk, spacy\n",
        "\n",
        "# NLTK resources (quiet)\n",
        "for res in [\"vader_lexicon\", \"stopwords\", \"punkt\"]:\n",
        "    try:\n",
        "        nltk.data.find(f\"sentiment/{res}\") if res==\"vader_lexicon\" else nltk.data.find(f\"tokenizers/{res}\")\n",
        "    except LookupError:\n",
        "        nltk.download(res, quiet=True)\n",
        "\n",
        "# spaCy model (small)\n",
        "try:\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "except Exception:\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"spacy\", \"download\", \"en_core_web_sm\"])\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "print(\"✅ Setup complete\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72c7e1d5",
      "metadata": {
        "id": "72c7e1d5"
      },
      "source": [
        "\n",
        "## 1) Data — Download BBC dataset from Kaggle\n",
        "> **Option A:** Kaggle API inside Colab (recommended).  \n",
        "> **Option B:** Upload CSV/JSON manually, then run **1.3 Load**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e338b4b4",
      "metadata": {
        "id": "e338b4b4"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Kaggle setup (Colab) — uncomment in Colab only\n",
        "# !pip install kaggle -q\n",
        "# from google.colab import files\n",
        "# print(\"Upload kaggle.json (Kaggle account → Create New API Token).\")\n",
        "# uploaded = files.upload()\n",
        "# !mkdir -p ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "# print(\"✅ Kaggle API ready.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f6807311",
      "metadata": {
        "id": "f6807311"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Download competition files — uncomment in Colab only\n",
        "# !kaggle competitions download -c learn-ai-bbc -q\n",
        "# !unzip -o learn-ai-bbc.zip\n",
        "# !ls -la\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95f9c47e",
      "metadata": {
        "id": "95f9c47e"
      },
      "source": [
        "### 1.3 Load BBC data (auto-detect common filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "35d0025d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35d0025d",
        "outputId": "bccb564a-eed9-4a8c-ca6e-03f35b975922"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV candidates: ['sample_data/mnist_train_small.csv', 'sample_data/california_housing_train.csv', 'sample_data/california_housing_test.csv', 'sample_data/mnist_test.csv']\n",
            "JSON candidates: ['sample_data/anscombe.json']\n",
            "⚠️ Upload a CSV via the Colab sidebar, then re-run this cell.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import glob, pandas as pd, os\n",
        "\n",
        "csv_candidates = glob.glob(\"*.csv\") + glob.glob(\"**/*.csv\", recursive=True)\n",
        "json_candidates = glob.glob(\"*.json\") + glob.glob(\"**/*.json\", recursive=True)\n",
        "print(\"CSV candidates:\", csv_candidates[:10])\n",
        "print(\"JSON candidates:\", json_candidates[:10])\n",
        "\n",
        "df = None\n",
        "src_file = None\n",
        "for fname in csv_candidates:\n",
        "    try:\n",
        "        tmp = pd.read_csv(fname)\n",
        "        cols = [c.lower() for c in tmp.columns]\n",
        "        if any(k in cols for k in [\"text\",\"content\",\"article\"]) and any(k in cols for k in [\"category\",\"label\",\"class\"]):\n",
        "            df = tmp.copy()\n",
        "            src_file = fname\n",
        "            break\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "if df is None and os.path.exists(\"train.csv\"):\n",
        "    df = pd.read_csv(\"train.csv\")\n",
        "    src_file = \"train.csv\"\n",
        "\n",
        "if df is not None:\n",
        "    print(f\"Loaded: {src_file}, shape={df.shape}\")\n",
        "    print(df.head())\n",
        "else:\n",
        "    print(\"⚠️ Upload a CSV via the Colab sidebar, then re-run this cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d473dd43",
      "metadata": {
        "id": "d473dd43"
      },
      "source": [
        "### 1.4 Standardize columns, filter & sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8afa4b14",
      "metadata": {
        "id": "8afa4b14"
      },
      "outputs": [],
      "source": [
        "\n",
        "def find_col(cands, cols):\n",
        "    cols_lower = [c.lower() for c in cols]\n",
        "    for c in cands:\n",
        "        if c in cols_lower:\n",
        "            return cols[cols_lower.index(c)]\n",
        "    return None\n",
        "\n",
        "if df is not None:\n",
        "    text_col = find_col([\"text\",\"content\",\"article\",\"description\",\"headline\"], df.columns)\n",
        "    label_col = find_col([\"category\",\"label\",\"class\",\"topic\"], df.columns)\n",
        "    if text_col is None or label_col is None:\n",
        "        raise ValueError(\"Could not infer text/label columns. Rename them to contain 'text' and 'category'.\")\n",
        "\n",
        "    df = df.dropna(subset=[text_col, label_col]).copy()\n",
        "    df = df[df[text_col].str.len() > 30]  # substantial text\n",
        "    df = df.rename(columns={text_col: \"content\", label_col: \"category\"})\n",
        "    if len(df) > 2000:\n",
        "        df = df.sample(2000, random_state=42)\n",
        "    print(df[\"category\"].value_counts())\n",
        "    df.to_csv(\"newsbot_dataset.csv\", index=False)\n",
        "    print(\"✅ Saved standardized dataset as newsbot_dataset.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3751fe2b",
      "metadata": {
        "id": "3751fe2b"
      },
      "source": [
        "## 2) Module 1 — Real-World NLP Application Context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34a83fde",
      "metadata": {
        "id": "34a83fde"
      },
      "source": [
        "\n",
        "**Use case:** Media monitoring dashboard for editors/PR teams to route and summarize news.  \n",
        "**Users:** Editors, PR/brand teams, analysts, researchers.  \n",
        "**Value:** Fast categorization, entity tracking, sentiment monitoring, pattern discovery.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5dec9ad5",
      "metadata": {
        "id": "5dec9ad5"
      },
      "source": [
        "## 3) Module 2 — Text Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c3209111",
      "metadata": {
        "id": "c3209111"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re, nltk\n",
        "from nltk.corpus import stopwords\n",
        "stop_set = set(stopwords.words(\"english\"))\n",
        "\n",
        "def clean_text(s: str) -> str:\n",
        "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" \", s)\n",
        "    s = re.sub(r\"<.*?>\", \" \", s)\n",
        "    s = re.sub(r\"[^A-Za-z0-9\\s']\", \" \", s)\n",
        "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def spacy_lemma(doc):\n",
        "    return \" \".join(t.lemma_.lower() for t in nlp(doc) if t.is_alpha and t.lemma_.lower() not in stop_set)\n",
        "\n",
        "if 'df' in globals() and df is not None:\n",
        "    df[\"clean\"] = df[\"content\"].apply(clean_text)\n",
        "    sample_n = min(2000, len(df))\n",
        "    df_lemma = df.sample(sample_n, random_state=42).copy()\n",
        "    df_lemma[\"lemma\"] = df_lemma[\"clean\"].apply(spacy_lemma)\n",
        "    df_lemma = df_lemma.dropna(subset=[\"lemma\"])\n",
        "    print(df_lemma[[\"category\",\"lemma\"]].head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c834fc3a",
      "metadata": {
        "id": "c834fc3a"
      },
      "source": [
        "## 4) Module 3 — TF‑IDF & Top Terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0b6018cd",
      "metadata": {
        "id": "0b6018cd"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "if 'df_lemma' in globals():\n",
        "    tfidf = TfidfVectorizer(max_features=20000, ngram_range=(1,2), min_df=2)\n",
        "    X = tfidf.fit_transform(df_lemma[\"lemma\"])\n",
        "    y = df_lemma[\"category\"].values\n",
        "    feature_names = np.array(tfidf.get_feature_names_out())\n",
        "\n",
        "    top_terms = {}\n",
        "    for cat in pd.Series(y).unique():\n",
        "        idx = np.where(y == cat)[0]\n",
        "        class_mean = X[idx].mean(axis=0).A1\n",
        "        top_idx = class_mean.argsort()[-15:][::-1]\n",
        "        top_terms[cat] = feature_names[top_idx].tolist()\n",
        "\n",
        "    for cat, terms in top_terms.items():\n",
        "        print(f\"\\nTop terms for {cat}:\")\n",
        "        print(\", \".join(terms))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4cdf221",
      "metadata": {
        "id": "a4cdf221"
      },
      "source": [
        "## 5) Module 4 — POS Distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7b6084be",
      "metadata": {
        "id": "7b6084be"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import Counter\n",
        "import itertools, pandas as pd\n",
        "\n",
        "def pos_counts(texts, max_docs=120):\n",
        "    c = Counter()\n",
        "    for t in itertools.islice(texts, max_docs):\n",
        "        for tok in nlp(t):\n",
        "            if not tok.is_space:\n",
        "                c[tok.pos_] += 1\n",
        "    total = sum(c.values()) or 1\n",
        "    return {k: v/total for k,v in c.items()}\n",
        "\n",
        "if 'df_lemma' in globals():\n",
        "    pos_by_cat = {cat: pos_counts(grp[\"lemma\"].tolist(), 100) for cat, grp in df_lemma.groupby(\"category\")}\n",
        "    pos_df = pd.DataFrame(pos_by_cat).fillna(0).sort_index()\n",
        "    display(pos_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae4db8da",
      "metadata": {
        "id": "ae4db8da"
      },
      "source": [
        "## 6) Module 5 — Syntax & SVO Samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "9ecf852e",
      "metadata": {
        "id": "9ecf852e"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_svo(doc):\n",
        "    subj, verb, obj_ = None, None, None\n",
        "    for tok in doc:\n",
        "        if tok.dep_ in (\"nsubj\",\"nsubjpass\"):\n",
        "            subj = tok.text\n",
        "            verb = tok.head.lemma_\n",
        "        if tok.dep_ in (\"dobj\",\"pobj\",\"attr\",\"dative\"):\n",
        "            obj_ = tok.text\n",
        "    if subj and verb and obj_:\n",
        "        return (subj, verb, obj_)\n",
        "    return None\n",
        "\n",
        "if 'df_lemma' in globals():\n",
        "    samples = []\n",
        "    for txt in df_lemma[\"lemma\"].head(40):\n",
        "        trip = extract_svo(nlp(txt))\n",
        "        if trip:\n",
        "            samples.append(trip)\n",
        "    print(samples[:12])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4276e4b7",
      "metadata": {
        "id": "4276e4b7"
      },
      "source": [
        "## 7) Module 6 — Sentiment by Category"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5a005c22",
      "metadata": {
        "id": "5a005c22"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "if 'df_lemma' in globals():\n",
        "    df_lemma[\"sentiment\"] = df_lemma[\"lemma\"].apply(lambda s: sia.polarity_scores(s)[\"compound\"])\n",
        "    print(df_lemma.groupby(\"category\")[\"sentiment\"].agg([\"mean\",\"median\",\"count\"]).sort_values(\"mean\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfdd0b86",
      "metadata": {
        "id": "cfdd0b86"
      },
      "source": [
        "## 8) Module 7 — Classifiers & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "72d49238",
      "metadata": {
        "id": "72d49238"
      },
      "outputs": [],
      "source": [
        "\n",
        "models = {\n",
        "    \"LogReg\": LogisticRegression(max_iter=2000),\n",
        "    \"LinearSVC\": LinearSVC(),\n",
        "    \"MultinomialNB\": MultinomialNB()\n",
        "}\n",
        "\n",
        "if 'df_lemma' in globals():\n",
        "    X_train, X_test, y_train, y_test = train_test_split(df_lemma[\"lemma\"], df_lemma[\"category\"], test_size=0.2, random_state=42, stratify=df_lemma[\"category\"])\n",
        "\n",
        "    results = {}\n",
        "    for name, clf in models.items():\n",
        "        pipe = Pipeline([(\"tfidf\", TfidfVectorizer(max_features=30000, ngram_range=(1,2), min_df=2)),\n",
        "                         (\"clf\", clf)])\n",
        "        pipe.fit(X_train, y_train)\n",
        "        preds = pipe.predict(X_test)\n",
        "        acc = accuracy_score(y_test, preds)\n",
        "        f1 = f1_score(y_test, preds, average=\"macro\")\n",
        "        results[name] = {\"accuracy\": acc, \"macro_f1\": f1, \"model\": pipe}\n",
        "        print(f\"{name}: accuracy={acc:.3f}, macro_f1={f1:.3f}\")\n",
        "\n",
        "    best = max(results.items(), key=lambda kv: kv[1][\"macro_f1\"])[0]\n",
        "    print(\"Best model:\", best)\n",
        "    print(\"\\nClassification report:\\n\", classification_report(y_test, results[best][\"model\"].predict(X_test)))\n",
        "    cm = confusion_matrix(y_test, results[best][\"model\"].predict(X_test), labels=sorted(y_test.unique()))\n",
        "\n",
        "    import matplotlib.pyplot as plt, numpy as np\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(cm, aspect='auto')\n",
        "    plt.title(\"Confusion Matrix (best model)\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.xticks(range(len(sorted(y_test.unique()))), sorted(y_test.unique()), rotation=45)\n",
        "    plt.yticks(range(len(sorted(y_test.unique()))), sorted(y_test.unique()))\n",
        "    plt.colorbar()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b48937e2",
      "metadata": {
        "id": "b48937e2"
      },
      "source": [
        "## 9) Module 8 — NER (PERSON/ORG/GPE/DATE/MONEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "5c4c5714",
      "metadata": {
        "id": "5c4c5714"
      },
      "outputs": [],
      "source": [
        "\n",
        "from collections import defaultdict, Counter\n",
        "import pandas as pd\n",
        "\n",
        "if 'df_lemma' in globals():\n",
        "    ent_counts = defaultdict(Counter)\n",
        "    for cat, grp in df_lemma.groupby(\"category\"):\n",
        "        for text in grp[\"lemma\"].head(60):\n",
        "            for e in nlp(text).ents:\n",
        "                if e.label_ in {\"PERSON\",\"ORG\",\"GPE\",\"DATE\",\"MONEY\"}:\n",
        "                    ent_counts[cat][e.label_] += 1\n",
        "    ent_df = pd.DataFrame(ent_counts).fillna(0).astype(int)\n",
        "    display(ent_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "026159b3",
      "metadata": {
        "id": "026159b3"
      },
      "source": [
        "\n",
        "## 10) Business Insights\n",
        "- **Routing:** classifier sends articles to the right desk; CM shows overlap (e.g., Tech vs Business).  \n",
        "- **Entities:** frequent **ORG/PERSON** per category → beats and alerts.  \n",
        "- **Sentiment:** track tone by category; anomalies can be newsworthy.  \n",
        "- **Style:** POS/Dependency reveal writing patterns across desks.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
